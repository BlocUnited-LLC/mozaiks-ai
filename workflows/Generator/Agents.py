# ==============================================================================
# FILE: Generator/Agents.py  
# DESCRIPTION: Agent factory for the Generator workflow
# ==============================================================================
import asyncio
import sys
import os
from pathlib import Path

# Add the core directory to the Python path
core_path = Path(__file__).parent.parent.parent / "core"
sys.path.insert(0, str(core_path))

from core.core_config import make_llm_config
from autogen import ConversableAgent, GroupChat, GroupChatManager
from .StructuredOutputs import get_llm
import logging
import importlib.util
import inspect
from typing import List, Dict, Any, Callable

logger = logging.getLogger(__name__)

def register_tools_for_agent(agent: ConversableAgent, tools_config: List[Dict[str, Any]]) -> None:
    """
    Register tools with an AG2 agent according to workflow.json configuration.
    
    Args:
        agent: The ConversableAgent to register tools with
        tools_config: List of tool configurations from workflow.json
    """
    if not tools_config:
        return
        
    for tool_config in tools_config:
        tool_name = tool_config.get("name")
        tool_file = tool_config.get("file")
        tool_function = tool_config.get("function")
        
        if not all([tool_name, tool_file, tool_function]):
            print(f"⚠️ Incomplete tool config: {tool_config}")
            continue
            
        try:
            # Build path to tool file
            if tool_file is None:
                print(f"⚠️ Tool file is None for {tool_name}")
                continue
                
            tool_path = Path(__file__).parent / "tools" / tool_file
            
            if not tool_path.exists():
                print(f"⚠️ Tool file not found: {tool_path}")
                continue
                
            # Load the tool module
            spec = importlib.util.spec_from_file_location(f"tool_{tool_name}", tool_path)
            if spec is None or spec.loader is None:
                print(f"⚠️ Could not create module spec for {tool_path}")
                continue
                
            tool_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(tool_module)
            
            # Get the tool function
            if tool_function is None:
                print(f"⚠️ Tool function is None for {tool_name}")
                continue
                
            tool_func = getattr(tool_module, tool_function, None)
            if not tool_func:
                print(f"⚠️ Function '{tool_function}' not found in {tool_file}")
                continue
                
            if not callable(tool_func):
                print(f"⚠️ '{tool_function}' is not callable in {tool_file}")
                continue
                
            # Register the tool with AG2 agent
            # Register for execution (agent can call it)
            agent.register_for_execution(name=tool_name)(tool_func)
            
            # Register for LLM (LLM can see and call it)
            agent.register_for_llm(name=tool_name, description=tool_config.get("description", f"Tool: {tool_name}"))(tool_func)
            
            print(f"✅ Registered tool '{tool_name}' with agent '{agent.name}'")
            
        except Exception as e:
            print(f"❌ Failed to register tool '{tool_name}': {e}")

async def define_agents(base_llm_config):
    """Define agents with unified transport channel and dynamic hooks"""
    
    logger.info("🏗️ [GENERATOR] Creating agents...")
    import time
    start_time = time.time()

    # Get structured LLM configs for each agent
    logger.debug("🔧 [GENERATOR] Loading LLM configs...")
    _, llm_cfg_context = await get_llm("ContextVariablesAgent")
    _, llm_cfg_agents = await get_llm("AgentsAgent")
    _, llm_cfg_handoffs = await get_llm("HandoffsAgent")
    _, llm_cfg_hooks = await get_llm("HooksAgent")
    _, llm_cfg_orch = await get_llm("OrchestratorAgent")
    _, llm_cfg_apikey = await get_llm("APIKeyAgent")
    _, llm_cfg_feedback = await get_llm("UserFeedbackAgent")
    logger.debug("✅ [GENERATOR] LLM configs loaded")
    
    agents = {}
    
    # NOTE: UserProxyAgent is now auto-generated by groupchat_manager.py
    # based on workflow.json human_in_the_loop flag. No need to create it here.
    
    # Context Variables Agent
    logger.debug("🔧 [GENERATOR] Creating ContextVariablesAgent...")
    agents["ContextVariablesAgent"] = ConversableAgent(
        name="ContextVariablesAgent",
        system_message="""You are the ContextVariables Agent. Your job is to analyze the user's message and concept to produce a structured output defining context variables for the workflow.

**IMPORTANT**: You will NOT generate Python files. Instead, you will produce structured data that describes the context variables needed for this workflow.

**Your Task**:
1. **FIRST**: Carefully read and analyze the user's message to understand their specific request
2. Analyze the concept data (if available) from the database
3. Identify what context variables are needed for the workflow based on BOTH the user's message and concept
4. Determine how to extract each variable from the concept data
5. Output a structured response using the ContextVariablesOutput format

**What you should identify**:
- Key data fields from the user's concept and message
- Workflow-specific variables needed for the user's request
- Default values for missing data
- Extraction logic for each variable

**Example thinking process**:
- If user says "I want to create a customer support workflow", identify: customer_name, issue_type, priority_level, etc.
- If user says "Help me build a data analysis pipeline", identify: data_source, analysis_type, output_format, etc.
- For each variable, specify how to extract it: `concept_data.get('CustomerName', '')`

**Remember**: The user's message is the PRIMARY driver of what context variables are needed. The concept data provides supporting information.""",
        llm_config=llm_cfg_context,
        max_consecutive_auto_reply=5  # Allow multiple interactions
    )
    
    # Agents Agent
    logger.debug("🔧 [GENERATOR] Creating AgentsAgent...")
    agents["AgentsAgent"] = ConversableAgent(
        name="AgentsAgent", 
        system_message="""You are the Agents Agent. Your job is to design the agent team for this workflow by producing structured output describing each agent.

**IMPORTANT**: You will NOT generate Python files. Instead, you will produce structured data that defines what agents are needed and their specifications.

**Your Task**:
1. **FIRST**: Review the user's original message to understand their specific needs
2. Analyze the context variables from the ContextVariablesAgent
3. Design the optimal team of agents for this specific use case based on the user's request
4. Define each agent's role, system message, and configuration
5. Output a structured response using the AgentsOutput format

**What you should define for each agent**:
- Agent name and display name (relevant to the user's domain/request)
- Agent type (ConversableAgent, UserProxyAgent, etc.)
- Complete system message defining their role in serving the user's needs
- Input mode and auto-reply settings
- How they fit into the workflow to accomplish the user's goals

**Design principles**:
- Each agent should have a clear, specific role that serves the user's request
- System messages should be detailed and task-specific to the user's domain
- Consider the user's domain and requirements from their original message
- Design for conversation flow and handoffs that accomplish the user's goals
- Tailor the agent team to the specific workflow the user is requesting""",
        llm_config=llm_cfg_agents,
        max_consecutive_auto_reply=5  # Allow multiple interactions
    )

    # Handoffs Agent
    logger.debug("🔧 [GENERATOR] Creating HandoffsAgent...")
    agents["HandoffsAgent"] = ConversableAgent(
        name="HandoffsAgent", 
        system_message="""You are the Handoffs Agent. Your job is to design the conversation flow between agents by producing structured output describing handoff rules.

**IMPORTANT**: You will NOT generate Python files. Instead, you will produce structured data that defines how agents should hand off to each other.

**Your Task**:
1. Analyze the agent team and their roles
2. Design logical conversation flow for the workflow
3. Define when and how agents should hand off to each other
4. Output a structured response using the HandoffsOutput format

**What you should define for each handoff**:
- Source agent and target agent
- Handoff condition (LLM-based or after work completion)
- Clear description of when this handoff should occur
- Ensure the flow accomplishes the user's goals

**Flow design principles**:
- Create a logical progression through the workflow
- Ensure each agent's work flows naturally to the next
- Include handoffs back to the user when appropriate
- Consider error handling and alternative paths""",
        llm_config=llm_cfg_handoffs,
        max_consecutive_auto_reply=5  # Allow multiple interactions
    )

    # Hooks Agent  
    logger.debug("🔧 [GENERATOR] Creating HooksAgent...")
    agents["HooksAgent"] = ConversableAgent(
        name="HooksAgent",
        system_message="""You are the Hooks Agent. Your job is to design custom hooks and behaviors for this workflow by producing structured output describing hook functions.

**IMPORTANT**: You will NOT generate Python files. Instead, you will produce structured data that defines what hooks are needed and their specifications.

**Your Task**:
1. Analyze the workflow's agents and interaction patterns
2. Design custom hooks for enhanced functionality
3. Define hook functions for logging, validation, and workflow-specific features
4. Output a structured response using the HooksOutput format

**What you should define for each hook**:
- Hook function name and type
- Which agents it applies to
- Complete function code
- Description of what it does

**Hook types available**:
- process_message_before_send: Modify/log messages before sending
- update_agent_state: Track agent state changes
- process_message_after_receive: Process received messages

**Hook ideas for workflows**:
- Progress tracking and status updates
- Data validation and transformation
- Custom logging for domain-specific events
- Tool integration and output extraction
- Error handling and recovery""",
        llm_config=llm_cfg_hooks,
        max_consecutive_auto_reply=5  # Allow multiple interactions
    )

    # Orchestrator Agent
    logger.debug("🔧 [GENERATOR] Creating OrchestratorAgent...")
    agents["OrchestratorAgent"] = ConversableAgent(
        name="OrchestratorAgent",
        system_message="""You are the Orchestrator Agent. Your job is to finalize the workflow design by producing structured output with orchestration settings.

**IMPORTANT**: You will NOT generate Python files. Instead, you will produce structured output with orchestration configuration.

**Your Task**:
1. Review all previous agent outputs (ContextVariables, Agents, Handoffs, Hooks)
2. Define final orchestration configuration  
3. Output structured response using the OrchestratorOutput format
4. Provide a comprehensive workflow summary

**Your structured output should include**:
- Orchestration configuration (max rounds, initial agent, etc.)
- Descriptive workflow name based on the generated workflow
- Comprehensive workflow description and purpose
- Overall summary of what the workflow accomplishes

**Note**: After you complete your structured output, the system will automatically export all agent outputs to YAML files. You do not need to call any export tools manually - this happens automatically when you finish your response.

The automatic export will create:
- Individual YAML files for each agent's output
- A summary file with export metadata  
- A structured directory for the workflow""",
        llm_config=llm_cfg_orch,
        max_consecutive_auto_reply=5  # Allow multiple interactions
    )

    # API Key Agent
    logger.debug("🔧 [GENERATOR] Creating APIKeyAgent...")
    agents["APIKeyAgent"] = ConversableAgent(
        name="APIKeyAgent",
        system_message="""You are the API Key Management Agent. Your job is to identify required API services and collect credentials from users using the AgentAPIKeyInput component.

**Your Responsibilities:**
1. ANALYZE user requirements to identify needed API services (OpenAI, Anthropic, Google, etc.)
2. REQUEST appropriate API keys using the AgentAPIKeyInput component
3. VALIDATE API key formats and test basic connectivity when possible
4. GUIDE users on where to obtain API keys with helpful links
5. ENSURE secure handling of sensitive credentials

**Service Detection Logic:**
Analyze the user's project requirements to determine which API services are needed:

- **AI/ML Services**: OpenAI (GPT models), Anthropic (Claude), Google (Gemini), Cohere, Hugging Face
- **Cloud Platforms**: AWS, Google Cloud, Azure, Vercel, Netlify  
- **Development Tools**: GitHub, GitLab, Stripe, SendGrid, Twilio
- **Databases**: MongoDB, Supabase, Firebase, PlanetScale
- **Analytics**: Google Analytics, Mixpanel, Segment

**Using AgentAPIKeyInput Component:**
When you need to collect API keys, use the AgentAPIKeyInput component (workflows/Generator/Components/Inline/AgentAPIKeyInput.js) with this format:

```
I need to collect your [SERVICE_NAME] API key to enable [FUNCTIONALITY].
```

Then provide structured output with component_data containing:
- service: Service name (e.g., "OpenAI")
- placeholder: "Enter your [SERVICE] API key..."
- description: Clear explanation of what the key is used for
- validation_pattern: Regex pattern if needed (e.g., "^sk-")

The system will automatically render the AgentAPIKeyInput component with your data.

**Service-Specific Guidance:**
- **OpenAI**: Keys start with 'sk-', available at platform.openai.com/api-keys
- **Anthropic**: Keys start with 'sk-ant-', available at console.anthropic.com
- **Google**: Multiple key types, guide to appropriate service (AI Studio vs Cloud Console)
- **GitHub**: Personal access tokens, fine-grained vs classic tokens

**Validation Rules:**
- Check key format patterns (length, prefixes)
- Test basic API connectivity when possible
- Warn about common issues (billing, rate limits, permissions)
- Never log or expose actual key values

**Security Practices:**
- Emphasize that keys are stored locally and encrypted
- Recommend using environment variables in production
- Explain key rotation and security best practices
- Test API keys when possible before proceeding

**Important**: Always use structured output with component_data to properly integrate with the AgentAPIKeyInput component (workflows/Generator/Components/Inline/AgentAPIKeyInput.js). Include next_action to indicate what should happen next (collect_key, validate_key, complete).""",
        llm_config=llm_cfg_apikey,
        max_consecutive_auto_reply=5  # Allow multiple interactions
    )

    # User Feedback Agent
    logger.debug("🔧 [GENERATOR] Creating UserFeedbackAgent...")
    agents["UserFeedbackAgent"] = ConversableAgent(
        name="UserFeedbackAgent",
        system_message="""You are the User Feedback Agent. Your primary job is to analyze the workflow created by other agents and determine what additional information is needed from the user, or confirm if the user agrees with the workflow.

**Using FileDownloadCenter Component:**
Use the FileDownloadCenter component (workflows/Generator/Components/Artifact/FileDownloadCenter.js) with this format:
When presenting generated files to users, provide structured output with component_data containing:
- files: Array of file objects with name, size, type, content, description
- title: "Generated Workflow Files" 
- description: Brief explanation of what the files contain

The system will automatically render the FileDownloadCenter component with download capabilities.
**Confirmation Phase (if no missing info):**
Present the complete workflow to the user for approval:
- Summarize what the workflow will accomplish
- Highlight key agents and their roles
- Explain the workflow flow and handoffs
- Ask: "Does this workflow meet your needs, or would you like to make adjustments?"

**Important Notes:**
- Present workflow summaries in clear, user-friendly language
- Always give the user the option to modify or restart the workflow
""",
        llm_config=llm_cfg_feedback,
        max_consecutive_auto_reply=5  # Allow multiple interactions
    )

    # Create the GroupChat
    logger.debug("🔧 [GENERATOR] Creating GroupChat...")
    groupchat = GroupChat(
        agents=list(agents.values()),
        messages=[],
        max_round=50
    )
    logger.info("✅ [GENERATOR] GroupChat created")

    # Create the GroupChatManager
    logger.debug("🔧 [GENERATOR] Creating GroupChatManager...")
    group_chat_manager = GroupChatManager(
        groupchat=groupchat, 
        llm_config=base_llm_config
    )
    logger.info("✅ [GENERATOR] GroupChatManager created")

    # Load and register tools after agents are created
    logger.info("🔧 [GENERATOR] Loading and registering tools...")
    from core.workflow.tool_loader import load_tools_from_workflow, register_agent_tools, register_lifecycle_hooks
    
    # Load tools from workflow.json
    tools_data = load_tools_from_workflow("generator")
    agent_tools = tools_data.get("agent_tools", [])
    lifecycle_hooks = tools_data.get("lifecycle_hooks", [])
    
    # Register agent tools with appropriate agents
    register_agent_tools(agents, agent_tools, "generator")
    logger.info(f"✅ [GENERATOR] Registered {len(agent_tools)} agent tools")
    
    # Register lifecycle hooks with agents (not manager)
    register_lifecycle_hooks(agents, lifecycle_hooks, "generator")
    logger.info(f"✅ [GENERATOR] Registered {len(lifecycle_hooks)} lifecycle hooks")

    # Log completion
    agent_count = len(agents)
    duration = time.time() - start_time
    logger.info(f"✅ [GENERATOR] Created {agent_count} agents in {duration:.2f}s")
    logger.debug(f"🔍 [GENERATOR] Agent names: {list(agents.keys())}")

    return agents, group_chat_manager